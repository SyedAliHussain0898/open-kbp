{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BGZduk0-bBP"
      },
      "source": [
        "<center>    \n",
        "<h3>American Association of Physicists in Medicine</h3>    \n",
        "<h3>Grand Challenge 2020</h3>\n",
        "<h3>OpenKBP</h3>\n",
        "<hr>\n",
        "<h1>Introduction for Google Colab</h1>\n",
        "<h3>January 25, 2022</h3>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbR496TRsFH9"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_kqJ8A1osFH9"
      },
      "source": [
        "Before running this notebook, we need to get the repo which contains the data. The download should be quick as it's a sercer-to-server process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vUrrkNE0sFH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5e0bbd9-cf17-4105-ec36-0a56870c17c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'open-kbp' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# Get the repo\n",
        "repo_dir = 'open-kbp'\n",
        "!git clone https://github.com/ababier/open-kbp.git {repo_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UdSmuUvmt8q4"
      },
      "outputs": [],
      "source": [
        "# Add repo to path\n",
        "import sys\n",
        "sys.path.append(repo_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "repo_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ihMT6sobqnuv",
        "outputId": "33391207-6b01-4f2b-cbf3-fac5a7b01b31"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'open-kbp'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHjRqM0guVnB"
      },
      "source": [
        "Also let's install the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FVu2PjKQs7zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "184121c6-1263-422f-c057-50360c30dcd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting black==22.12.0 (from -r open-kbp/requirements.txt (line 1))\n",
            "  Using cached black-22.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (52 kB)\n",
            "Collecting h5py==3.8.0 (from -r open-kbp/requirements.txt (line 2))\n",
            "  Using cached h5py-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting isort==5.11.4 (from -r open-kbp/requirements.txt (line 3))\n",
            "  Using cached isort-5.11.4-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jupyter==1.0.0 (from -r open-kbp/requirements.txt (line 4))\n",
            "  Using cached jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
            "Collecting keras==2.11.0 (from -r open-kbp/requirements.txt (line 5))\n",
            "  Using cached keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting matplotlib==3.6.3 (from -r open-kbp/requirements.txt (line 6))\n",
            "  Using cached matplotlib-3.6.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting more_itertools==9.0.0 (from -r open-kbp/requirements.txt (line 7))\n",
            "  Using cached more_itertools-9.0.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting numpy==1.24.1 (from -r open-kbp/requirements.txt (line 8))\n",
            "  Using cached numpy-1.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting pandas==1.5.3 (from -r open-kbp/requirements.txt (line 9))\n",
            "  Using cached pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pylint==2.15.10 (from -r open-kbp/requirements.txt (line 10))\n",
            "  Using cached pylint-2.15.10-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting scikit-learn==1.2.1 (from -r open-kbp/requirements.txt (line 11))\n",
            "  Using cached scikit_learn-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy==1.10.0 (from -r open-kbp/requirements.txt (line 12))\n",
            "  Using cached scipy-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting seaborn==0.12.2 (from -r open-kbp/requirements.txt (line 13))\n",
            "  Using cached seaborn-0.12.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.5 Requires-Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <3.7\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.11.0 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.19.0rc0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.11.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r open-kbp/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOa3thQo8z_q"
      },
      "source": [
        "Import all necessary packages for the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "vnvwlt8r80eR",
        "outputId": "2af6fe37-cb0a-487a-ff22-42cde5de4508",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.engine'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-8843185b6915>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprovided_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprovided_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdose_evaluation_class\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoseEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprovided_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprovided_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/open-kbp/provided_code/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdose_evaluation_class\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDoseEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mnetwork_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictionModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_paths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/open-kbp/provided_code/network_functions.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprovided_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_loader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mprovided_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_architectures\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDefineDoseFromCT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mprovided_code\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse_vector_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/open-kbp/provided_code/network_architectures.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_tensor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAveragePooling3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mConv3DTranspose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSpatialDropout3D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_normalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.engine'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# %tensorflow_version 2.x #  This ensures you use the newest version of tensorflow\n",
        "%tensorflow_version 2.x # Use tensorflow 2\n",
        "\n",
        "\n",
        "# Import provided classes and functions\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "from provided_code.data_loader import DataLoader\n",
        "from provided_code.dose_evaluation_class import DoseEvaluator\n",
        "from provided_code.network_functions import PredictionModel\n",
        "from provided_code.utils import get_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSmow21srjCI"
      },
      "source": [
        "The functions loaded from _provided\\_code_ are written for this competition, and you can access them via the file\n",
        "explorer on the left hand side of the Colab window. You're welcome to change them as much as\n",
        "you'd like. If you use Google Drive now, keep in mind, however, that on Colab any changes you make to the files in your Google Drive will only be recognized by Colab when the _Runtime_ is restarted via the Restart\n",
        "\n",
        " Runtime option in the top toolbar. If you implement a neural network, we urge you to you start with the provided\n",
        " network architecture and network functions. The neural network we provide is only meant to be a template, and will not\n",
        " be a competitive model without some significant modifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNRX4LzGsFIA"
      },
      "source": [
        "# Data loading\n",
        "Before we run anything, first define the paths where the provided data is stored and where the results (e.g., models, predictions) should be saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFJEIaar-d2r",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# Define project directories\n",
        "primary_directory = Path(repo_dir).resolve()  # directory where everything is stored\n",
        "provided_data_dir = primary_directory / \"provided-data\"\n",
        "training_data_dir = provided_data_dir / \"train-pats\"\n",
        "validation_data_dir = provided_data_dir / \"validation-pats\"\n",
        "testing_data_dir = provided_data_dir / \"test-pats\"\n",
        "results_dir = primary_directory / \"results\"  # where any data generated by this code (e.g., predictions, models) are stored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dHWwaHnzsFIB"
      },
      "source": [
        "Name the model. This name will be used to label directories containing the results that the model generates. Also,\n",
        "define how many epochs the model should be trained for. It will likely take a large number of epochs (e.g., 100-200)\n",
        "to get good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcHtM2JVsFIB"
      },
      "outputs": [],
      "source": [
        "test_time = False  # Only change this to True when the model has been fully tuned on the validation set\n",
        "prediction_name = \"baseline\"  # Name model to train and number of epochs to train it for\n",
        "num_epochs = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2z8m-4vZsFIC"
      },
      "source": [
        "Retrieve the paths for all patient directories in the training set and seperate them into a list of paths for training\n",
        "a model and another for hold-out testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luFCBkQksFIC",
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "outputs": [],
      "source": [
        "# Prepare the data directory\n",
        "training_plan_paths = get_paths(training_data_dir)  # gets the path of each plan's directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnRlu2MWsFID"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ThepV9jXsFID"
      },
      "source": [
        "Initialize a data loader for the training set data, and use it to initialize a prediction model object. Call the\n",
        "train_model method to train the model for the predefined number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM-W0nJIsFID"
      },
      "outputs": [],
      "source": [
        "# Train a model\n",
        "data_loader_train = DataLoader(training_plan_paths)\n",
        "dose_prediction_model_train = PredictionModel(data_loader_train, results_dir, prediction_name,  \"train\")\n",
        "dose_prediction_model_train.train_model(epochs=num_epochs, save_frequency=1, keep_model_history=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3ISL2hsLsFID"
      },
      "source": [
        "Note that during training we will only keep models that are __save_frequency * keep_model_history__ epochs back from the\n",
        "current epoch. We do this because models are very large (~1 GB).\n",
        "\n",
        "Now that the model is trained we can use it to predict the dose for a set of hold-out patients from the validation or\n",
        "testing set. The code block below gets the paths of all plans in the hold out set you selected earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IasmJpqsFIE"
      },
      "outputs": [],
      "source": [
        "# Define hold out set\n",
        "hold_out_data_dir = validation_data_dir if test_time is False else testing_data_dir\n",
        "stage_name, _ = hold_out_data_dir.stem.split(\"-\")\n",
        "hold_out_plan_paths = get_paths(hold_out_data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSMeJWlPsFIE"
      },
      "source": [
        "# Model testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjtEq3bfsFIF"
      },
      "source": [
        "# Saving results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kGYxma5xsFIF"
      },
      "source": [
        "Once you're happy with your dose distributions you can zip up the predictions with the code block below. The zipped file\n",
        "will contain the dose distributions for the validation set. It can be uploaded directly to CodaLab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeIBNHRAsFIF"
      },
      "outputs": [],
      "source": [
        "# Zip dose to submit\n",
        "submission_dir = results_dir / \"submissions\"\n",
        "submission_dir.mkdir(exist_ok=True)\n",
        "submission_zipfile = shutil.make_archive(\n",
        "    str(submission_dir / prediction_name),\n",
        "    \"zip\",\n",
        "    dose_prediction_model_hold_out.prediction_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia6IqwJdsFIG"
      },
      "source": [
        "You can save the model and submission to Google Drive or download them (on the left hand panel, click `...` -> `Download`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fyJTsjTh4OO"
      },
      "outputs": [],
      "source": [
        "# Mount your personal google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x3ab-5QsFIG"
      },
      "outputs": [],
      "source": [
        "# Save the submission\n",
        "submissions_on_drive = Path('/content/drive/MyDrive/open-kbp-subissions')\n",
        "submissions_on_drive.mkdir(exist_ok=True)\n",
        "shutil.copy(submission_zipfile, submissions_on_drive / f'{prediction_name}.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tGcZhgXh4OO"
      },
      "source": [
        "> Note that the model will be lost once you close the Colab session. You can download or save what's in the `results` folder."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "open-kbp-introduction.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "PyCharm (open-kbp-competition)",
      "language": "python",
      "name": "pycharm-3642c69f"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}