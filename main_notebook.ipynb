{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BGZduk0-bBP"
      },
      "source": [
        "<center>    \n",
        "<h3>American Association of Physicists in Medicine</h3>    \n",
        "<h3>Grand Challenge 2020</h3>\n",
        "<h3>OpenKBP</h3>\n",
        "<hr>\n",
        "<h1>Introduction for Google Colab</h1>\n",
        "<h3>January 25, 2022</h3>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbR496TRsFH9"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_kqJ8A1osFH9"
      },
      "source": [
        "Before running this notebook, we need to get the repo which contains the data. The download should be quick as it's a sercer-to-server process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vUrrkNE0sFH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e08e196-455c-4ec7-d52e-4f151ebf6295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'open-kbp'...\n",
            "remote: Enumerating objects: 4264, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 4264 (delta 40), reused 30 (delta 29), pack-reused 4208 (from 1)\u001b[K\n",
            "Receiving objects: 100% (4264/4264), 523.57 MiB | 16.54 MiB/s, done.\n",
            "Resolving deltas: 100% (1605/1605), done.\n",
            "Updating files: 100% (4016/4016), done.\n"
          ]
        }
      ],
      "source": [
        "# Get the repo\n",
        "repo_dir = 'open-kbp'\n",
        "!git clone https://github.com/ababier/open-kbp.git {repo_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UdSmuUvmt8q4"
      },
      "outputs": [],
      "source": [
        "# Add repo to path\n",
        "import sys\n",
        "sys.path.append(repo_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHjRqM0guVnB"
      },
      "source": [
        "Also let's install the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FVu2PjKQs7zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "befa74dd-5e77-47a1-88ab-ced31812ecbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting black==22.12.0 (from -r open-kbp/requirements.txt (line 1))\n",
            "  Downloading black-22.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.3/52.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h5py==3.8.0 (from -r open-kbp/requirements.txt (line 2))\n",
            "  Downloading h5py-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting isort==5.11.4 (from -r open-kbp/requirements.txt (line 3))\n",
            "  Downloading isort-5.11.4-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jupyter==1.0.0 (from -r open-kbp/requirements.txt (line 4))\n",
            "  Downloading jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
            "Collecting keras==2.11.0 (from -r open-kbp/requirements.txt (line 5))\n",
            "  Downloading keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting matplotlib==3.6.3 (from -r open-kbp/requirements.txt (line 6))\n",
            "  Downloading matplotlib-3.6.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting more_itertools==9.0.0 (from -r open-kbp/requirements.txt (line 7))\n",
            "  Downloading more_itertools-9.0.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting numpy==1.24.1 (from -r open-kbp/requirements.txt (line 8))\n",
            "  Downloading numpy-1.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting pandas==1.5.3 (from -r open-kbp/requirements.txt (line 9))\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pylint==2.15.10 (from -r open-kbp/requirements.txt (line 10))\n",
            "  Downloading pylint-2.15.10-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting scikit-learn==1.2.1 (from -r open-kbp/requirements.txt (line 11))\n",
            "  Downloading scikit_learn-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy==1.10.0 (from -r open-kbp/requirements.txt (line 12))\n",
            "  Downloading scipy-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting seaborn==0.12.2 (from -r open-kbp/requirements.txt (line 13))\n",
            "  Downloading seaborn-0.12.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "\u001b[31mERROR: Ignored the following versions that require a different python version: 1.21.2 Requires-Python >=3.7,<3.11; 1.21.3 Requires-Python >=3.7,<3.11; 1.21.4 Requires-Python >=3.7,<3.11; 1.21.5 Requires-Python >=3.7,<3.11; 1.21.6 Requires-Python >=3.7,<3.11; 1.6.2 Requires-Python >=3.7,<3.10; 1.6.3 Requires-Python >=3.7,<3.10; 1.7.0 Requires-Python >=3.7,<3.10; 1.7.1 Requires-Python >=3.7,<3.10; 1.7.2 Requires-Python >=3.7,<3.11; 1.7.3 Requires-Python >=3.7,<3.11; 1.8.0 Requires-Python >=3.8,<3.11; 1.8.0rc1 Requires-Python >=3.8,<3.11; 1.8.0rc2 Requires-Python >=3.8,<3.11; 1.8.0rc3 Requires-Python >=3.8,<3.11; 1.8.0rc4 Requires-Python >=3.8,<3.11; 1.8.1 Requires-Python >=3.8,<3.11; 1.9.5 Requires-Python >=2.7, !=3.0.*, !=3.1.*, !=3.2.*, !=3.3.*, <3.7\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.11.0 (from versions: 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.0.post1, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0, 2.19.0rc0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.11.0\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r open-kbp/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOa3thQo8z_q"
      },
      "source": [
        "Import all necessary packages for the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnvwlt8r80eR",
        "outputId": "9ba3a852-81b2-440b-ffe1-387888b1ce4b",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "# %tensorflow_version 2.x #  This ensures you use the newest version of tensorflow\n",
        "%tensorflow_version 2.x # Use tensorflow 2\n",
        "\n",
        "# Import provided classes and functions\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "from provided_code.data_loader import DataLoader\n",
        "from provided_code.dose_evaluation_class import DoseEvaluator\n",
        "from provided_code.network_functions import PredictionModel\n",
        "from provided_code.utils import get_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSmow21srjCI"
      },
      "source": [
        "The functions loaded from _provided\\_code_ are written for this competition, and you can access them via the file\n",
        "explorer on the left hand side of the Colab window. You're welcome to change them as much as\n",
        "you'd like. If you use Google Drive now, keep in mind, however, that on Colab any changes you make to the files in your Google Drive will only be recognized by Colab when the _Runtime_ is restarted via the Restart\n",
        "\n",
        " Runtime option in the top toolbar. If you implement a neural network, we urge you to you start with the provided\n",
        " network architecture and network functions. The neural network we provide is only meant to be a template, and will not\n",
        " be a competitive model without some significant modifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNRX4LzGsFIA"
      },
      "source": [
        "# Data loading\n",
        "Before we run anything, first define the paths where the provided data is stored and where the results (e.g., models, predictions) should be saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFJEIaar-d2r",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# Define project directories\n",
        "primary_directory = Path(repo_dir).resolve()  # directory where everything is stored\n",
        "provided_data_dir = primary_directory / \"provided-data\"\n",
        "training_data_dir = provided_data_dir / \"train-pats\"\n",
        "validation_data_dir = provided_data_dir / \"validation-pats\"\n",
        "testing_data_dir = provided_data_dir / \"test-pats\"\n",
        "results_dir = primary_directory / \"results\"  # where any data generated by this code (e.g., predictions, models) are stored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dHWwaHnzsFIB"
      },
      "source": [
        "Name the model. This name will be used to label directories containing the results that the model generates. Also,\n",
        "define how many epochs the model should be trained for. It will likely take a large number of epochs (e.g., 100-200)\n",
        "to get good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcHtM2JVsFIB"
      },
      "outputs": [],
      "source": [
        "test_time = False  # Only change this to True when the model has been fully tuned on the validation set\n",
        "prediction_name = \"baseline\"  # Name model to train and number of epochs to train it for\n",
        "num_epochs = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2z8m-4vZsFIC"
      },
      "source": [
        "Retrieve the paths for all patient directories in the training set and seperate them into a list of paths for training\n",
        "a model and another for hold-out testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luFCBkQksFIC",
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "outputs": [],
      "source": [
        "# Prepare the data directory\n",
        "training_plan_paths = get_paths(training_data_dir)  # gets the path of each plan's directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnRlu2MWsFID"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ThepV9jXsFID"
      },
      "source": [
        "Initialize a data loader for the training set data, and use it to initialize a prediction model object. Call the\n",
        "train_model method to train the model for the predefined number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM-W0nJIsFID"
      },
      "outputs": [],
      "source": [
        "# Train a model\n",
        "data_loader_train = DataLoader(training_plan_paths)\n",
        "dose_prediction_model_train = PredictionModel(data_loader_train, results_dir, prediction_name,  \"train\")\n",
        "dose_prediction_model_train.train_model(epochs=num_epochs, save_frequency=1, keep_model_history=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3ISL2hsLsFID"
      },
      "source": [
        "Note that during training we will only keep models that are __save_frequency * keep_model_history__ epochs back from the\n",
        "current epoch. We do this because models are very large (~1 GB).\n",
        "\n",
        "Now that the model is trained we can use it to predict the dose for a set of hold-out patients from the validation or\n",
        "testing set. The code block below gets the paths of all plans in the hold out set you selected earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IasmJpqsFIE"
      },
      "outputs": [],
      "source": [
        "# Define hold out set\n",
        "hold_out_data_dir = validation_data_dir if test_time is False else testing_data_dir\n",
        "stage_name, _ = hold_out_data_dir.stem.split(\"-\")\n",
        "hold_out_plan_paths = get_paths(hold_out_data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSMeJWlPsFIE"
      },
      "source": [
        "# Model testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aYroaHSNsFIE"
      },
      "source": [
        "We start by making a new data loader for the held-out set, and use it to predict (and save) a\n",
        "set of out-of-sample dose distributions. Note that we change the mode of the data loader to 'dose_prediction' to\n",
        "load only the data needed to make a prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR4cqYDOsFIE",
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "outputs": [],
      "source": [
        "# Predict dose for the held out set\n",
        "data_loader_hold_out = DataLoader(hold_out_plan_paths)\n",
        "dose_prediction_model_hold_out = PredictionModel(data_loader_hold_out, results_dir, model_name=prediction_name, stage=stage_name)\n",
        "dose_prediction_model_hold_out.predict_dose(epoch=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9cvDd66xsFIF",
        "pycharm": {
          "is_executing": false
        }
      },
      "source": [
        "Load each predicted dose distribution and evaluate it against the ground truth using the\n",
        "competition metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ8nSXgGsFIF"
      },
      "outputs": [],
      "source": [
        " # Evaluate dose metrics\n",
        "data_loader_hold_out_eval = DataLoader(hold_out_plan_paths)\n",
        "prediction_paths = get_paths(dose_prediction_model_hold_out.prediction_dir, extension=\"csv\")\n",
        "hold_out_prediction_loader = DataLoader(prediction_paths)\n",
        "dose_evaluator = DoseEvaluator(data_loader_hold_out_eval, hold_out_prediction_loader)\n",
        "\n",
        "# print out scores if data was left for a hold out set\n",
        "if not data_loader_hold_out_eval.patient_paths:\n",
        "    print(\"No patient information was given to calculate metrics\")\n",
        "else:\n",
        "    dose_evaluator.evaluate()\n",
        "    dvh_score, dose_score = dose_evaluator.get_scores()\n",
        "    print(f\"For this out-of-sample test on {stage_name}:\\n\\tthe DVH score is {dvh_score:.3f}\\n\\tthe dose score is {dose_score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjtEq3bfsFIF"
      },
      "source": [
        "# Saving results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kGYxma5xsFIF"
      },
      "source": [
        "Once you're happy with your dose distributions you can zip up the predictions with the code block below. The zipped file\n",
        "will contain the dose distributions for the validation set. It can be uploaded directly to CodaLab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeIBNHRAsFIF"
      },
      "outputs": [],
      "source": [
        "# Zip dose to submit\n",
        "submission_dir = results_dir / \"submissions\"\n",
        "submission_dir.mkdir(exist_ok=True)\n",
        "submission_zipfile = shutil.make_archive(\n",
        "    str(submission_dir / prediction_name),\n",
        "    \"zip\",\n",
        "    dose_prediction_model_hold_out.prediction_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia6IqwJdsFIG"
      },
      "source": [
        "You can save the model and submission to Google Drive or download them (on the left hand panel, click `...` -> `Download`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhGALCfguvcN"
      },
      "outputs": [],
      "source": [
        "# Mount your personal google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x3ab-5QsFIG"
      },
      "outputs": [],
      "source": [
        "# Save the submission\n",
        "submissions_on_drive = Path('/content/drive/MyDrive/open-kbp-subissions')\n",
        "submissions_on_drive.mkdir(exist_ok=True)\n",
        "shutil.copy(submission_zipfile, submissions_on_drive / f'{prediction_name}.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V_9YP1RuvcO"
      },
      "source": [
        "> Note that the model will be lost once you close the Colab session. You can download or save what's in the `results` folder."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "open-kbp-introduction.ipynb",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "PyCharm (open-kbp-competition)",
      "language": "python",
      "name": "pycharm-3642c69f"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}