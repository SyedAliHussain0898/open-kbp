{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BGZduk0-bBP"
      },
      "source": [
        "<center>    \n",
        "<h3>American Association of Physicists in Medicine</h3>    \n",
        "<h3>Grand Challenge 2020</h3>\n",
        "<h3>OpenKBP</h3>\n",
        "<hr>\n",
        "<h1>Introduction for Google Colab</h1>\n",
        "<h3>January 25, 2022</h3>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbR496TRsFH9"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_kqJ8A1osFH9"
      },
      "source": [
        "Before running this notebook, we need to get the repo which contains the data. The download should be quick as it's a sercer-to-server process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vUrrkNE0sFH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e08e196-455c-4ec7-d52e-4f151ebf6295"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'open-kbp'...\n",
            "remote: Enumerating objects: 4264, done.\u001b[K\n",
            "remote: Counting objects: 100% (56/56), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 4264 (delta 40), reused 30 (delta 29), pack-reused 4208 (from 1)\u001b[K\n",
            "Receiving objects: 100% (4264/4264), 523.57 MiB | 16.54 MiB/s, done.\n",
            "Resolving deltas: 100% (1605/1605), done.\n",
            "Updating files: 100% (4016/4016), done.\n"
          ]
        }
      ],
      "source": [
        "# Get the repo\n",
        "repo_dir = 'open-kbp'\n",
        "!git clone https://github.com/ababier/open-kbp.git {repo_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UdSmuUvmt8q4"
      },
      "outputs": [],
      "source": [
        "# Add repo to path\n",
        "import sys\n",
        "sys.path.append(repo_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHjRqM0guVnB"
      },
      "source": [
        "Also let's install the required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "FVu2PjKQs7zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39338522-7fad-47d7-a412-19ed8c337e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting black==22.12.0 (from -r open-kbp/requirements.txt (line 1))\n",
            "  Using cached black-22.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (52 kB)\n",
            "Collecting h5py==3.8.0 (from -r open-kbp/requirements.txt (line 2))\n",
            "  Using cached h5py-3.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
            "Collecting isort==5.11.4 (from -r open-kbp/requirements.txt (line 3))\n",
            "  Using cached isort-5.11.4-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting jupyter==1.0.0 (from -r open-kbp/requirements.txt (line 4))\n",
            "  Using cached jupyter-1.0.0-py2.py3-none-any.whl.metadata (995 bytes)\n",
            "Collecting keras==2.11.0 (from -r open-kbp/requirements.txt (line 5))\n",
            "  Using cached keras-2.11.0-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting matplotlib==3.6.3 (from -r open-kbp/requirements.txt (line 6))\n",
            "  Using cached matplotlib-3.6.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting more_itertools==9.0.0 (from -r open-kbp/requirements.txt (line 7))\n",
            "  Using cached more_itertools-9.0.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting numpy==1.24.1 (from -r open-kbp/requirements.txt (line 8))\n",
            "  Using cached numpy-1.24.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Collecting pandas==1.5.3 (from -r open-kbp/requirements.txt (line 9))\n",
            "  Using cached pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pylint==2.15.10 (from -r open-kbp/requirements.txt (line 10))\n",
            "  Using cached pylint-2.15.10-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting scikit-learn==1.2.1 (from -r open-kbp/requirements.txt (line 11))\n",
            "  Using cached scikit_learn-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting scipy==1.10.0 (from -r open-kbp/requirements.txt (line 12))\n",
            "  Using cached scipy-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Collecting seaborn==0.12.2 (from -r open-kbp/requirements.txt (line 13))\n",
            "  Using cached seaborn-0.12.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting tensorflow==2.15.0 (from -r open-kbp/requirements.txt (line 14))\n",
            "  Downloading tensorflow-2.15.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting tqdm==4.64.1 (from -r open-kbp/requirements.txt (line 15))\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from black==22.12.0->-r open-kbp/requirements.txt (line 1)) (8.1.8)\n",
            "Collecting mypy-extensions>=0.4.3 (from black==22.12.0->-r open-kbp/requirements.txt (line 1))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting pathspec>=0.9.0 (from black==22.12.0->-r open-kbp/requirements.txt (line 1))\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: platformdirs>=2 in /usr/local/lib/python3.11/dist-packages (from black==22.12.0->-r open-kbp/requirements.txt (line 1)) (4.3.6)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->-r open-kbp/requirements.txt (line 4)) (6.5.5)\n",
            "Collecting qtconsole (from jupyter==1.0.0->-r open-kbp/requirements.txt (line 4))\n",
            "  Downloading qtconsole-5.6.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->-r open-kbp/requirements.txt (line 4)) (6.1.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->-r open-kbp/requirements.txt (line 4)) (7.16.6)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->-r open-kbp/requirements.txt (line 4)) (6.17.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from jupyter==1.0.0->-r open-kbp/requirements.txt (line 4)) (7.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.6.3->-r open-kbp/requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.6.3->-r open-kbp/requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.6.3->-r open-kbp/requirements.txt (line 6)) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.6.3->-r open-kbp/requirements.txt (line 6)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.6.3->-r open-kbp/requirements.txt (line 6)) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.6.3->-r open-kbp/requirements.txt (line 6)) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.6.3->-r open-kbp/requirements.txt (line 6)) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib==3.6.3->-r open-kbp/requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==1.5.3->-r open-kbp/requirements.txt (line 9)) (2025.1)\n",
            "Collecting astroid<=2.14.0-dev0,>=2.12.13 (from pylint==2.15.10->-r open-kbp/requirements.txt (line 10))\n",
            "  Downloading astroid-2.13.5-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting mccabe<0.8,>=0.6 (from pylint==2.15.10->-r open-kbp/requirements.txt (line 10))\n",
            "  Downloading mccabe-0.7.0-py2.py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting tomlkit>=0.10.1 (from pylint==2.15.10->-r open-kbp/requirements.txt (line 10))\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting dill>=0.3.6 (from pylint==2.15.10->-r open-kbp/requirements.txt (line 10))\n",
            "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.1->-r open-kbp/requirements.txt (line 11)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.1->-r open-kbp/requirements.txt (line 11)) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (18.1.1)\n",
            "Collecting ml-dtypes~=0.2.0 (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14))\n",
            "  Downloading ml_dtypes-0.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (4.12.2)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14))\n",
            "  Downloading wrapt-1.14.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14)) (1.70.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14))\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.0->-r open-kbp/requirements.txt (line 14))\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
            "\u001b[31mERROR: Cannot install -r open-kbp/requirements.txt (line 14) and keras==2.11.0 because these package versions have conflicting dependencies.\u001b[0m\u001b[31m\n",
            "\u001b[0m\n",
            "The conflict is caused by:\n",
            "    The user requested keras==2.11.0\n",
            "    tensorflow 2.15.0 depends on keras<2.16 and >=2.15.0\n",
            "\n",
            "To fix this you could try to:\n",
            "1. loosen the range of package versions you've specified\n",
            "2. remove package versions to allow pip to attempt to solve the dependency conflict\n",
            "\n",
            "\u001b[31mERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -r open-kbp/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOa3thQo8z_q"
      },
      "source": [
        "Import all necessary packages for the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnvwlt8r80eR",
        "outputId": "9ba3a852-81b2-440b-ffe1-387888b1ce4b",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ],
      "source": [
        "# %tensorflow_version 2.x #  This ensures you use the newest version of tensorflow\n",
        "%tensorflow_version 2.x # Use tensorflow 2\n",
        "\n",
        "# Import provided classes and functions\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "from provided_code.data_loader import DataLoader\n",
        "from provided_code.dose_evaluation_class import DoseEvaluator\n",
        "from provided_code.network_functions import PredictionModel\n",
        "from provided_code.utils import get_paths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSmow21srjCI"
      },
      "source": [
        "The functions loaded from _provided\\_code_ are written for this competition, and you can access them via the file\n",
        "explorer on the left hand side of the Colab window. You're welcome to change them as much as\n",
        "you'd like. If you use Google Drive now, keep in mind, however, that on Colab any changes you make to the files in your Google Drive will only be recognized by Colab when the _Runtime_ is restarted via the Restart\n",
        "\n",
        " Runtime option in the top toolbar. If you implement a neural network, we urge you to you start with the provided\n",
        " network architecture and network functions. The neural network we provide is only meant to be a template, and will not\n",
        " be a competitive model without some significant modifications.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNRX4LzGsFIA"
      },
      "source": [
        "# Data loading\n",
        "Before we run anything, first define the paths where the provided data is stored and where the results (e.g., models, predictions) should be saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFJEIaar-d2r",
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": [
        "# Define project directories\n",
        "primary_directory = Path(repo_dir).resolve()  # directory where everything is stored\n",
        "provided_data_dir = primary_directory / \"provided-data\"\n",
        "training_data_dir = provided_data_dir / \"train-pats\"\n",
        "validation_data_dir = provided_data_dir / \"validation-pats\"\n",
        "testing_data_dir = provided_data_dir / \"test-pats\"\n",
        "results_dir = primary_directory / \"results\"  # where any data generated by this code (e.g., predictions, models) are stored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "dHWwaHnzsFIB"
      },
      "source": [
        "Name the model. This name will be used to label directories containing the results that the model generates. Also,\n",
        "define how many epochs the model should be trained for. It will likely take a large number of epochs (e.g., 100-200)\n",
        "to get good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcHtM2JVsFIB"
      },
      "outputs": [],
      "source": [
        "test_time = False  # Only change this to True when the model has been fully tuned on the validation set\n",
        "prediction_name = \"baseline\"  # Name model to train and number of epochs to train it for\n",
        "num_epochs = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "2z8m-4vZsFIC"
      },
      "source": [
        "Retrieve the paths for all patient directories in the training set and seperate them into a list of paths for training\n",
        "a model and another for hold-out testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luFCBkQksFIC",
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "outputs": [],
      "source": [
        "# Prepare the data directory\n",
        "training_plan_paths = get_paths(training_data_dir)  # gets the path of each plan's directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnRlu2MWsFID"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ThepV9jXsFID"
      },
      "source": [
        "Initialize a data loader for the training set data, and use it to initialize a prediction model object. Call the\n",
        "train_model method to train the model for the predefined number of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FM-W0nJIsFID"
      },
      "outputs": [],
      "source": [
        "# Train a model\n",
        "data_loader_train = DataLoader(training_plan_paths)\n",
        "dose_prediction_model_train = PredictionModel(data_loader_train, results_dir, prediction_name,  \"train\")\n",
        "dose_prediction_model_train.train_model(epochs=num_epochs, save_frequency=1, keep_model_history=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "3ISL2hsLsFID"
      },
      "source": [
        "Note that during training we will only keep models that are __save_frequency * keep_model_history__ epochs back from the\n",
        "current epoch. We do this because models are very large (~1 GB).\n",
        "\n",
        "Now that the model is trained we can use it to predict the dose for a set of hold-out patients from the validation or\n",
        "testing set. The code block below gets the paths of all plans in the hold out set you selected earlier.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IasmJpqsFIE"
      },
      "outputs": [],
      "source": [
        "# Define hold out set\n",
        "hold_out_data_dir = validation_data_dir if test_time is False else testing_data_dir\n",
        "stage_name, _ = hold_out_data_dir.stem.split(\"-\")\n",
        "hold_out_plan_paths = get_paths(hold_out_data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSMeJWlPsFIE"
      },
      "source": [
        "# Model testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "aYroaHSNsFIE"
      },
      "source": [
        "We start by making a new data loader for the held-out set, and use it to predict (and save) a\n",
        "set of out-of-sample dose distributions. Note that we change the mode of the data loader to 'dose_prediction' to\n",
        "load only the data needed to make a prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR4cqYDOsFIE",
        "pycharm": {
          "name": "#%% \n"
        }
      },
      "outputs": [],
      "source": [
        "# Predict dose for the held out set\n",
        "data_loader_hold_out = DataLoader(hold_out_plan_paths)\n",
        "dose_prediction_model_hold_out = PredictionModel(data_loader_hold_out, results_dir, model_name=prediction_name, stage=stage_name)\n",
        "dose_prediction_model_hold_out.predict_dose(epoch=num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "9cvDd66xsFIF",
        "pycharm": {
          "is_executing": false
        }
      },
      "source": [
        "Load each predicted dose distribution and evaluate it against the ground truth using the\n",
        "competition metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ8nSXgGsFIF"
      },
      "outputs": [],
      "source": [
        " # Evaluate dose metrics\n",
        "data_loader_hold_out_eval = DataLoader(hold_out_plan_paths)\n",
        "prediction_paths = get_paths(dose_prediction_model_hold_out.prediction_dir, extension=\"csv\")\n",
        "hold_out_prediction_loader = DataLoader(prediction_paths)\n",
        "dose_evaluator = DoseEvaluator(data_loader_hold_out_eval, hold_out_prediction_loader)\n",
        "\n",
        "# print out scores if data was left for a hold out set\n",
        "if not data_loader_hold_out_eval.patient_paths:\n",
        "    print(\"No patient information was given to calculate metrics\")\n",
        "else:\n",
        "    dose_evaluator.evaluate()\n",
        "    dvh_score, dose_score = dose_evaluator.get_scores()\n",
        "    print(f\"For this out-of-sample test on {stage_name}:\\n\\tthe DVH score is {dvh_score:.3f}\\n\\tthe dose score is {dose_score:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjtEq3bfsFIF"
      },
      "source": [
        "# Saving results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "kGYxma5xsFIF"
      },
      "source": [
        "Once you're happy with your dose distributions you can zip up the predictions with the code block below. The zipped file\n",
        "will contain the dose distributions for the validation set. It can be uploaded directly to CodaLab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FeIBNHRAsFIF"
      },
      "outputs": [],
      "source": [
        "# Zip dose to submit\n",
        "submission_dir = results_dir / \"submissions\"\n",
        "submission_dir.mkdir(exist_ok=True)\n",
        "submission_zipfile = shutil.make_archive(\n",
        "    str(submission_dir / prediction_name),\n",
        "    \"zip\",\n",
        "    dose_prediction_model_hold_out.prediction_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ia6IqwJdsFIG"
      },
      "source": [
        "You can save the model and submission to Google Drive or download them (on the left hand panel, click `...` -> `Download`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhGALCfguvcN"
      },
      "outputs": [],
      "source": [
        "# Mount your personal google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x3ab-5QsFIG"
      },
      "outputs": [],
      "source": [
        "# Save the submission\n",
        "submissions_on_drive = Path('/content/drive/MyDrive/open-kbp-subissions')\n",
        "submissions_on_drive.mkdir(exist_ok=True)\n",
        "shutil.copy(submission_zipfile, submissions_on_drive / f'{prediction_name}.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4V_9YP1RuvcO"
      },
      "source": [
        "> Note that the model will be lost once you close the Colab session. You can download or save what's in the `results` folder."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "open-kbp-introduction.ipynb",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "PyCharm (open-kbp-competition)",
      "language": "python",
      "name": "pycharm-3642c69f"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}